---
title: "Forecast Analysis"
format:
  html:
    embed-resources: true
editor: visual
---

## Problem set

You will be using your model_id, climatology and persistence in the analysis below. You will be using site_ids BARC and SUGG (since they have year-around data)

Resources:

<https://frec-5174.github.io/eco4cast-in-R-book/visualizing.html>

<https://frec-5174.github.io/eco4cast-in-R-book/process-model-forecast-evaluation.html>

Find your forecast scores here:

<https://radiantearth.github.io/stac-browser/#/external/raw.githubusercontent.com/eco4cast/neon4cast-ci/main/catalog/scores/collection.json>

climatology and persistenceRW are the null model_id
###Loading data
```{r}
library(arrow)
library(tidyverse)
library(ggplot2)

# all_results <- arrow::open_dataset("s3://anonymous@bio230014-bucket01/challenges/scores/parquet/project_id=neon4cast/duration=P1D/variable=temperature/model_id=lm_AT_WTL_WS?endpoint_override=sdsc.osn.xsede.org")
# myresults <- all_results |> dplyr::collect()
# myresults$model_id<-"myresults"
# 
# all_results_pers <- arrow::open_dataset("s3://anonymous@bio230014-bucket01/challenges/scores/parquet/project_id=neon4cast/duration=P1D/variable=temperature/model_id=persistenceRW?endpoint_override=sdsc.osn.xsede.org")
# persistance <- all_results_pers |> dplyr::collect()
# persistance$model_id<-"persistance"
# 
# all_results_clim <- arrow::open_dataset("s3://anonymous@bio230014-bucket01/challenges/scores/parquet/project_id=neon4cast/duration=P1D/variable=temperature/model_id=climatology?endpoint_override=sdsc.osn.xsede.org")
# climatology <- all_results_clim |> dplyr::collect()
# climatology$model_id<-"climatology"
# 
# allmodels<-rbind(myresults, persistance, climatology)
# allmodels$reference_datetime <- as.Date(allmodels$reference_datetime)
# allmodels$datetime <- as.Date(allmodels$datetime)
# allmodels$pub_datetime <- as.Date(allmodels$pub_datetime)
# allmodels$date <- as.Date(allmodels$date)


all_results <- arrow::open_dataset("s3://anonymous@bio230014-bucket01/challenges/scores/parquet/project_id=neon4cast/duration=P1D/variable=temperature?endpoint_override=sdsc.osn.xsede.org")
df_with_baselines <- all_results |> 
  filter(site_id %in% c("BARC", "SUGG"),
         reference_datetime > as_date("2024-03-01"), 
         model_id %in% c("lm_AT_WTL_WS", "climatology", "persistenceRW")) |> 
  collect()

df_with_baselines$reference_datetime <- as.Date(df_with_baselines$reference_datetime)
df_with_baselines$datetime <- as.Date(df_with_baselines$datetime)
df_with_baselines$pub_datetime <- as.Date(df_with_baselines$pub_datetime)
df_with_baselines$date <- as.Date(df_with_baselines$date)
```

### Question 1

Plot climatology, persistence, and your model for a single forecast day (one reference_datetime) on the same plot. Use geom_ribbon to plot the uncertainty, geom_line to plot the median, and geom_point to plot the observations.

```{r}

df_with_baselines %>% filter(reference_datetime == "2024-04-01") %>% 
  ggplot() + 
  geom_line(aes(x=datetime, y=mean, group=model_id, color=model_id)) + 
  geom_ribbon(aes(x=datetime, ymin=quantile10, ymax=quantile90, group=model_id, fill=model_id), alpha = 0.2) + 
  geom_line(aes(x=datetime, y=median, group=model_id, color=model_id), linetype=2)+ 
  facet_wrap(vars(site_id)) + 
  theme_bw() + 
  labs(x="Date Time", y = "Mean", title="Forecasts from 2024-04-01 with 80% CI", subtitle="Solid line = mean; Dashed line = median") + 
  geom_point(aes(x=datetime, y=observation), color="black", size=0.5)

```

### Question 2

Based on visual inspection of your plot, how do the median of model differ in how they represent the observations.

The median of the persistence model holds steadily at one value, despite ovservation values. The climatology median increases over the forecast horizon at both sites, which matches the linear trend of the observations. The median of climatology seems to be reverse of the observations at SUGG: the median goes up when observations go down, and vice versa. However, the values of the observations are broadly captured by the 80% CI. My model (lm_AT_WTL_WS) follows more closely the shape of the observed data but at the wrong values, so my CI does not capture the majority of observed values. 

### Question 3

Based on visual inspection of your plot, how does the uncertainty of each model forecasts differ in capacity to represent the observations.

Because the persistance model has the greatest uncertainty, it captures the most observed data values compared to the other models. While my model follows a similar shape to the data, most of the observed data are not captured by the confidence interval because the median values are off. The climatology model captures the majority of data points within the uncertainty bounds, even if the observed data do not follow the same trend shape as the model's median. 

### Question 4

Calculate the mean CRPS for the three models (averaged across all horizons, sites, and reference_datetimes). Which model has the lower score?

```{r}
df_with_baselines |> 
  select(model_id, crps, datetime, reference_datetime, site_id) |> 
  pivot_wider(names_from = model_id, values_from = crps) |> 
  na.omit() |> 
  pivot_longer(-c(datetime, reference_datetime, site_id), names_to = "model_id", values_to = "crps") |> 
  summarise(mean_crps = mean(crps), .by = c("model_id")) |> 
  ggplot(aes(x = model_id, y = mean_crps)) +
  geom_bar(stat="identity") + 
  theme_bw() + 
  labs(y="Mean CRPS", x="Model ID")
```

Climatology has the lowest mean CRPS, and my model performs even worse than the persistence model. 

### Question 5

Plot the mean CRPS vs horizon for all three models. How does the performance change as you forecast further in the future?

```{r}
df_with_baselines |> 
  mutate(horizon = as.numeric(datetime - reference_datetime)) |> 
  select(model_id, horizon, datetime, reference_datetime, crps, site_id) |> 
  pivot_wider(names_from = model_id, values_from = crps) |> 
  na.omit() |> 
  pivot_longer(-c(horizon, datetime, reference_datetime, site_id), names_to = "model_id", values_to = "crps") |> 
  summarize(mean_crps = mean(crps), .by = c("model_id", "horizon")) |> 
  ggplot(aes(x = horizon, y = mean_crps, color = model_id)) + 
  geom_line() + 
  theme_bw() + 
  labs(y="Mean CRPS", x="Horizon (days)")
```

Very early on in the forecast horizon, my model performs the best until about 6 days out from the start date. The persistence model also performs better than climatology for the first three days. After one week into the forecast horizon, the climatology model performs extraordinarily better than the other models. My model performs the worst after day 11 in the forecast horizon. 

### Question 6

Plot the mean CRPS separately for each site_id for all three models. How does performance differ between sites?

```{r}
df_with_baselines |> 
  select(model_id, crps, datetime, reference_datetime, site_id) |> 
  pivot_wider(names_from = model_id, values_from = crps) |> 
  na.omit() |> 
  pivot_longer(-c(datetime, reference_datetime, site_id), names_to = "model_id", values_to = "crps") |> 
  summarise(mean_crps = mean(crps), .by = c("model_id", "site_id")) |> 
  ggplot(aes(x = model_id, y = mean_crps)) +
  geom_bar(stat="identity") + 
  theme_bw() + 
  labs(y="Mean CRPS", x="Model ID") + 
  facet_wrap(vars(site_id))
```
All models perform better at BARC than SUGG, and climatology still performs the best at each site. My model performs the worst at each site, but the difference in mean CRPS between climatology and my model is slightly less for BARC (0.30) than for SUGG (0.39). 

### Question 7

Which forecasting best practices are addressed with your forecasts and the analysis above? See <https://frec-5174.github.io/eco4cast-in-R-book/best-practices.html>.
